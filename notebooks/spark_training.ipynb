{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7770bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from pyspark.sql import functions, SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6993277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/juan/proyectos/docker/learning-docker/basic-ml-proj'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Setting to parent directory\n",
    "os.chdir('./basic-ml-proj')\n",
    "\n",
    "# Printing current directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88042604",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A SparkSession consolidates SQLContext, HiveContext and StreamingContext into\n",
    "one entry point, simplifying interation between Spark and its different APIS.\n",
    "\n",
    "A SparkContext is an entry point to PySpark functionality used to communicate to\n",
    "the cluster and create an RDD accumulator and broadcast variables.\n",
    "\n",
    "What are RDDs (Resilient Distributed Dataset)?\n",
    "\n",
    "It is a inmutable distributed collection of objects. Each dataset in RDD is divided\n",
    "into logical partitions, which may be computed in different nodes of the cluster.\n",
    "\n",
    "Is a read only partitioned collection of records.\n",
    "\"\"\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"parquetFile\").getOrCreate() # Create a Spark session\n",
    "\n",
    "# Example data\n",
    "data = [\n",
    "    (\"James \", \"\", \"Smith\", \"36636\", \"M\", 3000),\n",
    "    (\"John \", \"\", \"Doe\", \"32636\", \"M\", 4000),\n",
    "    (\"Maria  \", \"Anne \", \"Smith\", \"16636\", \"F\", 3000),\n",
    "]\n",
    "\n",
    "columns = [\"firstname\", \"middlename\", \"lastname\", \"dob\", \"gender\", \"salary\"]\n",
    "\n",
    "# Creation of DataFrame\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13b85dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to path\n",
    "df.write.parquet('data/raw/dummy_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Transformations and Actions...\n",
    "\n",
    "A Transformation is a function that maps a RDD to another RDD.\n",
    "\n",
    "There are various kinds of transformations:\n",
    "\n",
    "1. Narrow Transformation: 1-1 function of parent to child (map(), filter()).\n",
    "2. Wide Transformation: Bijective function, all the child RDD partitions might depend on a single\n",
    "                        parent RDD partition (groupbyKey(), reducebyKey()).\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
